{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SUTD](data/sutd.png)\n",
    "# 50.040 Natural Language Processing (Summer 2020) Homework 1\n",
    "\n",
    "### **Due 5 June 2020, 5pm** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### STUDNET ID: 1002934\n",
    "\n",
    "### Name: Chloe Zheng \n",
    "\n",
    "### Students with whom you have discussed (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Word embeddings are dense vectors that represent words, and capable of capturing semantic and syntactic similarity, relation with other words, etc.\n",
    "We have introduced two approaches in the class to learn word embeddings: **Count-based** and **Prediction-based**. \n",
    "Here we will explore both approaches and learn *co-occurence matrices* word embeddings and *Word2Vec* word embeddings. Note that we use \"word embeddings\" and \"word vectors\" interchangeably.\n",
    "\n",
    "-------\n",
    "\n",
    "Before we start, you need to [download](http://mattmahoney.net/dc/text8.zip) the text8 dataset. Unzip the file and then put it under the \"data\" folder. The text8 dataset consists of one single line of long text. Please do not change the data unless you are requested to do so.\n",
    "\n",
    "Environment:\n",
    "- Python 3.5 or above\n",
    "- gensim \n",
    "- sklearn\n",
    "- numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Count-based word embeddings\n",
    "\n",
    "### Co-Occurrence\n",
    "\n",
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the *context window* surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window.\n",
    "\n",
    "**Example: Co-Occurrence with Fixed Window of n=1**:\n",
    "\n",
    "Document 1: \"learn and live\"\n",
    "\n",
    "Document 2: \"learn not and know not\"\n",
    "\n",
    "|     *    | and | know | learn | live | not |\n",
    "|----------|-----|------|-------|------|-----|\n",
    "| and      | 0   | 1    | 1     | 1    | 1   |\n",
    "| know     | 1   | 0    | 0     | 0    | 1   |\n",
    "| learn    | 1   | 0    | 0     | 0    | 1   |\n",
    "| live     | 1   | 0    | 0     | 0    | 0   |\n",
    "| not      | 1   | 1    | 1     | 0    | 0   |\n",
    "\n",
    "\n",
    "The rows or columns can be used as word vectors but they are usually too large (linear in the size of the vocabulary). Thus \n",
    "in the next step we need to run \"dimensionality reduction\" algorithms like PCA, SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct co-occurence matrix\n",
    "\n",
    "Before you start, please make sure you have downloaded the dataset \"text8\" in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(file_path, size=500000):\n",
    "    '''\n",
    "    params:\n",
    "        file_path --- str: path to your data file.\n",
    "        size --- int or str: the size of the corpus\n",
    "    return:\n",
    "        corpus --- list[str]: list of word strings.\n",
    "    '''\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "        if size=='all':\n",
    "            corpus = text.split()\n",
    "        else:\n",
    "            corpus = text.split()[:size]\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n",
      "<class 'list'>\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(r'./data/text8')\n",
    "print(corpus[0:10])\n",
    "\n",
    "print(type(corpus))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 [code]:\n",
    "Implement the function \"distinct_words\" that reads in \"corpus\" and returns distinct words that appeared in the corpus, the number of distinct words. \n",
    "\n",
    "Then, run the sanity check cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-57-2161637679ed>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-2161637679ed>\"\u001b[1;36m, line \u001b[1;32m22\u001b[0m\n\u001b[1;33m    num_corpus_words= len(corpus_words)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "\n",
    "def distinct_words(corpus):\n",
    "    \"\"\" \n",
    "    Determine a list of distinct words for the corpus.\n",
    "    Params:\n",
    "        corpus --- list[str]: list of words in the corpus\n",
    "    Return:\n",
    "        corpus_words --- list[str]: list of distinct words in the corpus; sort this list with built-in python function \"sorted\"\n",
    "        num_corpus_words --- int: number of distinct in the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = None\n",
    "    num_corpus_words = None\n",
    "    ### You may need to use \"set()\" to remove duplicate words.\n",
    "    ### YOUR CODE HERE (~2 lines)\n",
    "    \n",
    "    sort_corpus = sorted(corpus)\n",
    "#     for word in sort_corpus:\n",
    "#         if word not in corpus_words:\n",
    "#             corpus_words.append(word)\n",
    "            \n",
    "    num_corpus_words= len(corpus_words)\n",
    "\n",
    "    return corpus_words, num_corpus_words\n",
    "\n",
    "x = distinct_words(corpus)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Run this sanity check to check your implementation\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define toy corpus\n",
    "test_corpus = \"learn and live\".split() + \"learn not and know not\".split()\n",
    "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
    "\n",
    "# Correct answers\n",
    "ans_test_corpus_words = sorted(list(set(['learn','and','live','not','know'])))\n",
    "ans_num_corpus_words = len(ans_test_corpus_words)\n",
    "\n",
    "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
    "\n",
    "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 [code]: \n",
    "Implement \"compute_co_occurrence_matrix\" that reads in \"corpus\" and \"window_size\", and returns a co-occurence matrix and a word-to-index dictionary.\n",
    "\n",
    "Then, run the sanity check cell to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=1):\n",
    "    \"\"\" \n",
    "    Compute co-occurrence matrix for the given corpus and window_size (default of 1).\n",
    "    \n",
    "    Params:\n",
    "        corpus --- list[str]: list of words\n",
    "        window_size --- int: size of context window\n",
    "    Return:\n",
    "        M --- numpy array of shape (num_words, num_words)): \n",
    "              Co-occurence matrix of word counts. \n",
    "              The ordering of the words in the rows/columns should be the same as the ordering of the words \n",
    "              given by the distinct_words function.\n",
    "              \n",
    "        word2Ind --- dict: dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    ###    Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "    ###    number of co-occurring words.\n",
    "    ###    For example, if we take the sentence \"learn and live\" with window size of 2,\n",
    "    ###    \"learn\" will co-occur with \"and\", \"live\". \n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END OF YOUR CODE   \n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and get co-occurrence matrix\n",
    "test_corpus = \"learn not and know not\".split()\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
    "# Correct M and word2Ind\n",
    "M_test_ans = np.array( \n",
    "    [[0., 1., 0., 1.],\n",
    "     [1., 0., 0., 1.],\n",
    "     [0., 0., 0., 1.],\n",
    "     [1., 1., 1., 0.]])\n",
    "\n",
    "word2Ind_ans = {'and':0, 'know':1, 'learn':2,  'not':3}\n",
    "\n",
    "# check correct word2Ind\n",
    "assert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n",
    "\n",
    "# check correct M shape\n",
    "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
    "\n",
    "# Test correct M values\n",
    "for w1 in word2Ind_ans.keys():\n",
    "    idx1 = word2Ind_ans[w1]\n",
    "    for w2 in word2Ind_ans.keys():\n",
    "        idx2 = word2Ind_ans[w2]\n",
    "        student = M_test[idx1, idx2]\n",
    "        correct = M_test_ans[idx1, idx2]\n",
    "        if student != correct:\n",
    "            print(\"Correct M:\")\n",
    "            print(M_test_ans)\n",
    "            print(\"Your M: \")\n",
    "            print(M_test)\n",
    "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 [code]:\n",
    "Implement \"pca\" function below with python package sklearn.decomposition.PCA. For the use of PCA function, please refer to https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "Then, run the sanity check cell to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pca(X, k=2):\n",
    "    '''\n",
    "    A wrapper of the sklearn.decomposition.PCA function.\n",
    "    params:\n",
    "        X --- numpy array of shape (num_words, word_embedding_size)\n",
    "        k --- int: the number of principal components that we keep\n",
    "    return:\n",
    "        X_pca --- numpy array of shape (num_words, k)\n",
    "    '''\n",
    "    X_pca = None\n",
    "    \n",
    "    ### YOUR CODE HERE (~2 line)\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# only check that your M_reduced has the right dimensions.\n",
    "# ---------------------\n",
    "\n",
    "# Define toy corpus and run student code\n",
    "test_corpus = \"learn not and know not\".split()\n",
    "M_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
    "M_test_reduced = pca(M_test, k=2)\n",
    "\n",
    "# Test proper dimensions\n",
    "assert (M_test_reduced.shape[0] == 4), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 4)\n",
    "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
    "\n",
    "# Print Success\n",
    "print (\"-\" * 80)\n",
    "print(\"Passed All Tests!\")\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 [code]:\n",
    "Implement \"plot_embeddings\" function to visualize the word embeddings on a 2-D plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(X_pca, word2Ind, words):\n",
    "    \"\"\" \n",
    "    Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
    "        \n",
    "    params:\n",
    "        X_pca --- numpy array of shape (num_words , 2): numpy array of 2-d word embeddings\n",
    "        word2Ind --- dict: dictionary that maps words to indices\n",
    "        words --- list[str]: a list of words of which the embeddings we want to visualize\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### You may need to use \"plt.scatter\", \"plt.text\" and a for loop here\n",
    "    ### YOUR CODE HERE (~ 7 lines)\n",
    "\n",
    "    ### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Run this sanity check\n",
    "# Note that this not an exhaustive check for correctness.\n",
    "# The plot produced should look like the \"test solution plot\" depicted below. \n",
    "# ---------------------\n",
    "\n",
    "print (\"-\" * 80)\n",
    "print (\"Outputted Plot:\")\n",
    "\n",
    "X_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
    "word2Ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
    "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
    "plot_embeddings(X_test, word2Ind_plot_test, words)\n",
    "\n",
    "print (\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Test Plot Solution**</font>\n",
    "<br>\n",
    "<img src=\"data/test_plot.png\" width=40% style=\"float: left;\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Run This Cell to Produce Your Plot\n",
    "# ------------------------------\n",
    "corpus = read_corpus(r'./data/text8',100000)\n",
    "M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(corpus, window_size=4)\n",
    "M_reduced_co_occurrence = pca(M_co_occurrence, k=2)\n",
    "\n",
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
    "\n",
    "words = ['king','man','woman','women','queen','prince']\n",
    "plot_embeddings(M_normalized, word2Ind_co_occurrence, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prediction-based word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 [written]:\n",
    "Given a sentence \"I am interested in NLP\", what will be the context and target pairs in a CBOW/Skip-gram model if the window size is 1? Write your answer in the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 [code]:\n",
    "Complete the code in the function *create_word_batch*, which can be used to divide a single sequence of words into batches of words. \n",
    "\n",
    "For example, the word sequence [\"I\", \"like\", \"NLP\", \"So\", \"does\", \"he\"] can be divided into two batches, [\"I\", \"like\", \"NLP\"], [\"So\", \"does\", \"he\"], each with batch_size=3 words. It is more efficient to train word embedding on batches of word sequences rather than on a long single sequence. \n",
    "\n",
    "Then, run the sanity check cell to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_word_batch(words, batch_size=100):\n",
    "    '''\n",
    "    Split the words into batches\n",
    "    params:\n",
    "        words --- list[str]: a list of words\n",
    "        batch_size --- int: the number of words in a batch\n",
    "    return:\n",
    "        batch_words: list[list[str]]batches of words, list\n",
    "    '''\n",
    "    batch_words = []\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return batch_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Run this sanity check to check your implementation\n",
    "# --------------------------------------------------\n",
    "words_test = [\"I\", \"like\", \"NLP\", \"So\", \"does\", \"he\"]\n",
    "batch_size_test = 3\n",
    "\n",
    "ans = [[\"I\", \"like\", \"NLP\"],[\"So\", \"does\", \"he\"]]\n",
    "\n",
    "batch_words_test = create_word_batch(words_test,batch_size_test)\n",
    "\n",
    "assert ans == batch_words_test, 'your output does not match \"ans\"'\n",
    "print('passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 [code]:\n",
    "Use \"Word2Vec\" function to build a word2vec model. For the use of \"Word2Vec\" function, please ,refer to https://radimrehurek.com/gensim/models/word2vec.html. Please use the parameters we have set for you.\n",
    "\n",
    "It may take a few minutes to train the model.\n",
    "\n",
    "If you encounter \"UserWarning: C extension not loaded, training will be slow\", try to uninstall gensim first and then run \"pip install gensim==3.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whole_corpus = corpus = read_corpus(r'./data/text8', 'all')\n",
    "batch_words = create_word_batch(whole_corpus)\n",
    "\n",
    "size = 100\n",
    "min_count = 2\n",
    "window = 3\n",
    "sg = 1\n",
    "### YOUR CODE HERE (1 line)\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 [code]:\n",
    "Implement \"get_word2Ind\" function below.\n",
    "\n",
    "Then, run the sanity check cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_word2Ind(index2word):\n",
    "    '''\n",
    "    construct a dictionary that maps words to its index\n",
    "    \n",
    "    params:\n",
    "        index2word --- list[str]: list of words\n",
    "    return \n",
    "        word2index --- dict: keys are words, values are the corresponding indices\n",
    "    '''\n",
    "    word2index = dict()\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Run this sanity check to check your implementation\n",
    "# --------------------------------------------------\n",
    "i2w_test = ['I','love','it']\n",
    "ans_test = get_word2Ind(i2w_test)\n",
    "\n",
    "ans = {'I':0, 'love':1, 'it':2}\n",
    "assert ans == ans_test, 'your output did not match the correct answer.'\n",
    "print('passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to visualize the word embeddings of the first 300 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2Ind = get_word2Ind(model.wv.index2word)\n",
    "\n",
    "vocab = model.wv.vocab\n",
    "words_to_visualize = list(vocab.keys())[:300]\n",
    "\n",
    "vec_pca = pca(model.wv.vectors, 2)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_embeddings(vec_pca, word2Ind, words_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9:\n",
    "Find the most similar words for the given words \"dog\",\"car\",\"man\". You need to use \"model.wv.most_similar\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words =  ['dog', 'car', 'man'] \n",
    "\n",
    "### YOUR CODE HERE (~ 2 lines)\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 [written]:\n",
    "Run the code below and explain the results in the empty cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['london', 'japan'], negative=['england'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
