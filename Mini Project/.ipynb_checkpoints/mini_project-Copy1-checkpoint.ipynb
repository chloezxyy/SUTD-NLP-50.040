{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf_8OI34CdXU"
   },
   "source": [
    "<img src=\"sutd.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "## <center>50.040 Natural Language Processing, Summer 2020<center>\n",
    "<center>**Due 19 June 2020, 5pm** <center>\n",
    "Mini Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAN5MA5GCdXU"
   },
   "source": [
    "**Write your student ID and name**\n",
    "\n",
    "\n",
    "### STUDNET ID: 1002934\n",
    "\n",
    "### Name: Chloe Zheng \n",
    "\n",
    "### Students with whom you have discussed (if any):\n",
    "Lionell, Yi Xuan, Jia Yee, Yong Quan, Caleb, Bryan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0iF5uWcCdXV"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Language models are very useful for a wide range of applications, e.g., speech recognition and machine translation. Consider a sentence consisting of words $x_1, x_2, …, x_m$, where $m$ is the length of the sentence, the goal of language modeling is to model the probability of the sentence, where $m \\geq 1$, $x_i \\in V $ and $V$ is the vocabulary of the corpus:\n",
    "$$p(x_1, x_2, …, x_m)$$\n",
    "In this project, we are going to explore both statistical language model and neural language model on the [Wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) datasets. Download wikitext-2 word-level data and put it under the ``data`` folder.\n",
    "\n",
    "## Statistical  Language Model\n",
    "\n",
    "A simple way is to view words as independent random variables (i.e., zero-th order Markovian assumption). The joint probability can be written as:\n",
    "$$p(x_1, x_2, …, x_m)=\\prod_{i=1}^m p(x_i)$$\n",
    "However, this model ignores the word order information, to account for which, under the first-order Markovian assumption, the joint probability can be written as:\n",
    "$$p(x_0, x_1, x_2, …, x_m)= \\prod_{i=1}^{m}p(x_i \\mid x_{i-1})$$\n",
    "Under the second-order Markovian assumption, the joint probability can be written as:\n",
    "$$p(x_{-1}, x_0, x_1, x_2, …, x_m)= \\prod_{i=1}^{m}p(x_i \\mid x_{i-2}, x_{i-1})$$\n",
    "Similar to what we did in HMM, we will assume that $x_{-1}=START, x_0=START, x_m = STOP$ in this definition, where $START, STOP$ are special symbols referring to the start and the end of a sentence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5wC4lqKCdXW"
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "Let's use $count(u)$ to denote the number of times the unigram $u$ appears in the corpus, use $count(v, u)$ to denote the number of times the bigram $v, u$ appears in the corpus, and $count(w, v, u)$ the times the trigram $w, v, u$ appears in the corpus, $u \\in V \\cup STOP$ and $w, v \\in V \\cup START$.\n",
    "\n",
    "And the parameters of the unigram, bigram and trigram models can be obtained using maximum likelihood estimation (MLE).\n",
    "\n",
    "- In the unigram model, the parameters can be estimated as: $$p(u) = \\frac {count(u)}{c}$$, where $c$ is the total number of words in the corpus.\n",
    "- In the bigram model, the parameters can be estimated as:\n",
    "$$p(u \\mid v) = \\frac{count(v, u)}{count(v)}$$\n",
    "- In the trigram model, the parameters can be estimated as:\n",
    "$$p(u \\mid w, v) = \\frac{count(w, v, u)}{count(w, v)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "YdDGHLkkjC8k",
    "outputId": "02a96359-5b3b-4201-eabb-b057ffdc081f"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "  TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "  TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7dks8t0CdXW"
   },
   "source": [
    "### Smoothing the parameters\n",
    "Note, it is likely that many parameters of bigram and trigram models will be 0 because the relevant bigrams and trigrams involved do not appear in the corpus. If you don't have a way to handle these 0 probabilities, all the sentences that include such bigrams or trigrams will have probabilities of 0.\n",
    "\n",
    "We'll use a Add-k Smoothing method to fix this problem, the smoothed parameter can be estimated as:\n",
    "\\begin{equation}\n",
    "p_{add-k}(u)= \\frac{count(u)+k}{c+k|V^*|}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "p_{add-k}(u \\mid v)= \\frac{count(v, u)+k}{count(v)+k|V^*|}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "p_{add-k}(u \\mid w, v)= \\frac{count(w, v, u)+k}{count(w, v)+k|V^*|}\n",
    "\\end{equation}\n",
    "\n",
    "where $k \\in (0, 1)$ is the parameter of this approach, and $|V^*|$ is the size of the vocabulary $V^*$,here $V^*= V \\cup STOP$. One way to choose the value of $k$ is by\n",
    "optimizing the perplexity of the development set, namely to choose the value that minimizes the perplexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNVha_8UCdXX"
   },
   "source": [
    "### Perplexity\n",
    "\n",
    "Given a test set $D^{\\prime}$ consisting of sentences $X^{(1)}, X^{(2)}, …, X^{(|D^{\\prime}|)}$, each sentence $X^{(j)}$ consists of words $x_1^{(j)}, x_2^{(j)},…,x_{n_j}^{(j)}$, we can measure the probability of each sentence $s_i$, and the quality of the language model would be the probability it assigns to the entire set of test sentences, namely:\n",
    "\\begin{equation} \n",
    "\\prod_j^{D^{\\prime}}p(X^{(j)})\n",
    "\\end{equation}\n",
    "Let's define average log2 probability as:\n",
    "\\begin{equation} \n",
    "l=\\frac{1}{c^{\\prime}}\\sum_{j=1}^{|D^{\\prime}|}log_2p(X^{(j)})\n",
    "\\end{equation}\n",
    "$c^{\\prime}$ is the total number of words in the test set, $D^{\\prime}$ is the number of sentences. And the perplexity is defined as:\n",
    "\\begin{equation} \n",
    "perplexity=2^{-l}\n",
    "\\end{equation}\n",
    "\n",
    "The lower the perplexity, the better the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uT0qLdb9jC8u"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "import itertools\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OzkrkNbljC80",
    "outputId": "93168d18-d44b-4238-dd1d-f30a8a20d810"
   },
   "outputs": [],
   "source": [
    "\n",
    "#file = open('/content/drive/My Drive/Colab Notebooks/data/wikitext-2/wiki.train.tokens')\n",
    "\n",
    "with open('data/wikitext-2/wiki.train.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    train_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    train_sents = [s for s in train_sents if len(s)>0 and s[0] != '=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "dl6-Y6o-jC86",
    "outputId": "dee28717-118b-4249-8b41-2788803d2ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', ',', 'it', 'also', 'underwent', 'multiple', 'adjustments', ',', 'such', 'as', 'making', 'the', 'game', 'more', '<unk>', 'for', 'series', 'newcomers', '.', 'character', 'designer', '<unk>', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', ',', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', '.', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', 'the', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'n\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0vxL-WpCdXX"
   },
   "source": [
    "### Question 1 [code][written]\n",
    "1. Implement the function **\"compute_ngram\"** that computes n-grams in the corpus.\n",
    " (Do not take the START and STOP symbols into consideration for now.) \n",
    " For n=1,2,3, the number of unique n-grams should be **28910/577343/1344047**, respectively.\n",
    "2. List 10 most frequent unigrams, bigrams and trigrams as well as their counts.(Hint: use the built-in function .most_common in Counter class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOyoT-QiCdXY"
   },
   "outputs": [],
   "source": [
    "def compute_ngram(sents, n):\n",
    "    '''\n",
    "    Compute n-grams that appear in \"sents\".\n",
    "    param:\n",
    "        sents: list[list[str]] --- list of list of word strings\n",
    "        n: int --- \"n\" gram\n",
    "    return:\n",
    "        ngram_set: set{str} --- a set of n-grams (no duplicate elements)\n",
    "        ngram_dict: dict{ngram: counts} --- a dictionary that maps each ngram to its number occurence in \"sents\";\n",
    "        This dict contains the parameters of our ngram model. E.g. if n=2, ngram_dict={('a','b'):10, ('b','c'):13}\n",
    "        \n",
    "        You may need to use \"Counter\", \"tuple\" function here.\n",
    "    '''\n",
    "    ngram_set = None\n",
    "    ngram_dict = {}\n",
    "    \n",
    "    grams = []\n",
    "    \n",
    "    # iterate over outer loop \n",
    "    for sentence in sents:\n",
    "        for word in range(len(sentence) - n + 1):\n",
    "            for gram in range(n):\n",
    "                grams.append(sentence[word + gram])\n",
    "                \n",
    "            tuple_hold = tuple(grams)\n",
    "            ngram_dict.setdefault(tuple_hold, 0)\n",
    "            # empty list in ea iteration\n",
    "            grams = []\n",
    "            ngram_dict[tuple_hold] +=1\n",
    "                    \n",
    "    ngram_set = ngram_dict.keys()\n",
    "        \n",
    "    return ngram_set, ngram_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oJiaUSTRjC9E",
    "outputId": "bb48df58-7b83-4954-b45f-a9e6201d35f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28910\n"
     ]
    }
   ],
   "source": [
    "### ~28xxx\n",
    "unigram_set, unigram_dict = compute_ngram(train_sents, 1)\n",
    "print(len(unigram_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ohfOgKGijC9R",
    "outputId": "8ed8b12f-c28f-4b8d-ba87-8fbf5d628ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577343\n"
     ]
    }
   ],
   "source": [
    "### ~57xxxx 577343\n",
    "bigram_set, bigram_dict = compute_ngram(train_sents, 2)\n",
    "print(len(bigram_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "thKGtbDdjC9W",
    "outputId": "fbe2df21-b9bd-49f0-d333-8ffcdf0e959c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1344047\n"
     ]
    }
   ],
   "source": [
    "### ~134xxxx 1344047\n",
    "trigram_set, trigram_dict = compute_ngram(train_sents, 3)\n",
    "print(len(trigram_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "IQnQ92sTjC9Z",
    "outputId": "82557cb6-5f61-418b-91bc-e90b61379603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni [(('the',), 130519), ((',',), 99763), (('.',), 73388), (('of',), 56743), (('<unk>',), 53951), (('and',), 49940), (('in',), 44876), (('to',), 39462), (('a',), 36140), (('\"',), 28285)]\n",
      "\n",
      "\n",
      "bi [(('of', 'the'), 17242), (('in', 'the'), 11778), ((',', 'and'), 11643), (('.', 'the'), 11274), ((',', 'the'), 8024), (('<unk>', ','), 7698), (('to', 'the'), 6009), (('on', 'the'), 4495), (('the', '<unk>'), 4389), (('and', 'the'), 4331)]\n",
      "\n",
      "\n",
      "tri [((',', 'and', 'the'), 1393), ((',', '<unk>', ','), 950), (('<unk>', ',', '<unk>'), 901), (('one', 'of', 'the'), 866), (('<unk>', ',', 'and'), 819), (('.', 'however', ','), 775), (('<unk>', '<unk>', ','), 745), (('.', 'in', 'the'), 726), (('.', 'it', 'was'), 698), (('the', 'united', 'states'), 666)]\n"
     ]
    }
   ],
   "source": [
    "# List 10 most frequent unigrams, bigrams and trigrams as well as their counts.\n",
    "\n",
    "counts_uni = Counter(unigram_dict).most_common(10)\n",
    "\n",
    "counts_bi = Counter(bigram_dict).most_common(10)\n",
    "\n",
    "counts_tri = Counter(trigram_dict).most_common(10)\n",
    "\n",
    "print(\"uni\", counts_uni)\n",
    "print(\"\\n\\nbi\", counts_bi)\n",
    "print(\"\\n\\ntri\", counts_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVpoDgU7CdXb"
   },
   "source": [
    "### Question 2 [code][written]\n",
    "In this part, we take the START and STOP symbols into consideration. So we need to pad the **train_sents** as described in \"Statistical Language Model\" before we apply \"compute_ngram\" function. For example, given a sentence \"I like NLP\", in a bigram model, we need to pad it as \"START I like NLP STOP\", in a trigram model, we need to pad it as \"START START I like NLP STOP\".\n",
    "\n",
    "1. Implement the ``pad_sents function``.\n",
    "2. Pad ``train_sents``.\n",
    "3. Apply ``compute_ngram`` function to these padded sents. \n",
    "4. Implement ``ngram_prob`` function. Compute the probability for each n-gram in the variable **ngrams** according to Eq.(1)(2)(3) in **\"smoothing the parameters\"** .List down the n-grams that have 0 probability. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "kH8-AI3GjC9g",
    "outputId": "34830a71-165f-4cd0-ef8e-e41158a92a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'computer'], ['go', 'to'], ['have', 'had'], ['and', 'the'], ['can', 'sea'], ['a', 'number', 'of'], ['with', 'respect', 'to'], ['in', 'terms', 'of'], ['not', 'good', 'bad'], ['first', 'start', 'with']]\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "\n",
    "\n",
    "ngrams = list()\n",
    "with open('data/ngram.txt','r') as f:\n",
    "    for line in f:\n",
    "        ngrams.append(line.strip('\\n').split())\n",
    "print(ngrams)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3dCIrjNjC9m"
   },
   "outputs": [],
   "source": [
    "def pad_sent(sent, n):\n",
    "    if n > 1:\n",
    "        padded = [START for _ in range(n-1)]\n",
    "        padded.extend(sent)\n",
    "    else:\n",
    "        padded = sent\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCHLs3o1jC9r",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "START = '<START>'\n",
    "STOP = '<STOP>'\n",
    "###################################\n",
    "def pad_sents(sents, n):\n",
    "    '''\n",
    "    Pad the sents according to n.\n",
    "    params:\n",
    "        sents: list[list[str]] --- list of sentences.\n",
    "        n: int --- specify the padding type, 1-gram, 2-gram, or 3-gram.\n",
    "    return:\n",
    "        padded_sents: list[list[str]] --- list of padded sentences.\n",
    "    '''\n",
    "    padded_sents = [pad_sent(sent, n) for sent in sents]\n",
    "    print(padded_sents[0])\n",
    "     \n",
    "    ### END OF YOUR CODE\n",
    "    return padded_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "OKDwyz9tjC9w",
    "outputId": "cc11c7f7-b5dc-4729-c92b-eb2a4cb1b136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senjō', 'no', 'valkyria', '3', ':', '<unk>', 'chronicles', '(', 'japanese', ':', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media.vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', 'nameless', '\"', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '\"', '<unk>', 'raven', '\"', '.']\n",
      "['<START>', 'senjō', 'no', 'valkyria', '3', ':', '<unk>', 'chronicles', '(', 'japanese', ':', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media.vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', 'nameless', '\"', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '\"', '<unk>', 'raven', '\"', '.']\n",
      "['<START>', '<START>', 'senjō', 'no', 'valkyria', '3', ':', '<unk>', 'chronicles', '(', 'japanese', ':', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media.vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', 'nameless', '\"', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '\"', '<unk>', 'raven', '\"', '.']\n"
     ]
    }
   ],
   "source": [
    "uni_sents = pad_sents(train_sents, 1)\n",
    "bi_sents = pad_sents(train_sents, 2)\n",
    "tri_sents = pad_sents(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjTDmuNTjC9z"
   },
   "outputs": [],
   "source": [
    "unigram_set, unigram_dict = compute_ngram(uni_sents, 1)\n",
    "bigram_set, bigram_dict = compute_ngram(bi_sents, 2)\n",
    "trigram_set, trigram_dict = compute_ngram(tri_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hVKjF1k5jC93",
    "outputId": "c6a96d4a-e757-416f-84be-0c666cd65b23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28910, 580115, 1356254)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### (28xxx, 58xxxx, 136xxxx)\n",
    "len(unigram_set),len(bigram_set),len(trigram_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wzfhhTyJjC98",
    "outputId": "bade7035-0c21-4628-ca78-18e2e7f1e66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007146\n"
     ]
    }
   ],
   "source": [
    "### ~ 200xxxx; total number of words in wikitext-2.train\n",
    "num_words = sum([v for _,v in unigram_dict.items()])\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RbSIwJtjC-C"
   },
   "outputs": [],
   "source": [
    "def ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    prob = None\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ngram_tuple = tuple(ngram)\n",
    "\n",
    "    if len(ngram) == 1:\n",
    "        #prob = count(u) / c\n",
    "        # to find number of occurrence the unigram list appears from the unigram_dict\n",
    "        prob = unigram_dic.get(ngram_tuple, 0) / num_words\n",
    "        \n",
    "    elif len(ngram) == 2:\n",
    "        # count(v,u)/ count(v)\n",
    "        prob = bigram_dic.get(ngram_tuple, 0) / unigram_dic.get(tuple([ngram_tuple[0]]), 0)\n",
    "    \n",
    "    elif len(ngram) ==3:\n",
    "        #count(w,v,u)/count(w,v)\n",
    "        prob = trigram_dic.get(ngram_tuple, 0) / bigram_dic.get(tuple(ngram_tuple[0:2]), 0)\n",
    "        \n",
    "    ### END OF YOUR CODE\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "WKxsDoa8jC-H",
    "outputId": "3b42901d-f822-4044-ee05-49632b633cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['the', 'computer'], ['go', 'to'], ['have', 'had'], ['and', 'the'], ['can', 'sea'], ['a', 'number', 'of'], ['with', 'respect', 'to'], ['in', 'terms', 'of'], ['not', 'good', 'bad'], ['first', 'start', 'with'])\n",
      "weird ['the', 'computer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.960235674499498e-05"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ~9.96e-05\n",
    "print(tuple(ngrams))\n",
    "ngram_tuple = tuple(ngrams)\n",
    "print('weird', ngram_tuple[0])\n",
    "\n",
    "ngram_prob(ngrams[0], num_words,unigram_dict, bigram_dict, trigram_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "I8q8Rep_jC-N",
    "outputId": "d93da7fe-eed8-4724-dfb6-9a897a179a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams with 0 probability: ['can', 'sea']\n",
      "ngrams with 0 probability: ['not', 'good', 'bad']\n",
      "ngrams with 0 probability: ['first', 'start', 'with']\n"
     ]
    }
   ],
   "source": [
    "### List down the n-grams that have 0 probability. \n",
    "\n",
    "# iterate through ngrams\n",
    "for given in ngrams:\n",
    "#     print('give', given)\n",
    "    get_ngram_prob = ngram_prob(given, num_words,unigram_dict, bigram_dict, trigram_dict)\n",
    "    # get the ith ngram and put into get_ngram_prob\n",
    "    if get_ngram_prob == 0:\n",
    "        print('ngrams with 0 probability:', given)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kAezpJ9CdXd"
   },
   "source": [
    "### Question 3 [code][written]\n",
    "\n",
    "1. Implement ``smooth_ngram_prob`` function to estimate ngram probability with ``add-k`` smoothing technique. Compute the smoothed probabilities of each n-gram in the variable **\"ngrams\"** according to Eq.(1)(2)(3) in **\"smoothing the parameters\"** section.\n",
    "2. Implement ``perplexity`` function to compute the perplexity of the corpus \"**valid_sents**\" according to the Equations (4),(5),(6) in **perplexity** section. The computation of $p(X^{(j)})$ depends on the n-gram model you choose. If you choose 2-gram model, then you need to calculate $p(X^{(j)})$ based on Eq.(2) in **smoothing the parameter** section. Hint: convert probability to log probability.\n",
    "3. Try out different $k\\in [0.1, 0.3, 0.5, 0.7, 0.9]$ and different n-gram model ($n=1,2,3$). Find the n-gram model and $k$ that gives the best perplexity on \"**valid_sents**\" (smaller is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "qS7d0wUkjC-U",
    "outputId": "0710bc84-11c5-41bb-f5e7-cb6683c21920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homarus', 'gammarus', ',', 'known', 'as', 'the', 'european', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'atlantic', 'ocean', ',', 'mediterranean', 'sea', 'and', 'parts', 'of', 'the', 'black', 'sea', '.', 'it', 'is', 'closely', 'related', 'to', 'the', 'american', 'lobster', ',', 'h.', 'americanus', '.', 'it', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'in', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', '\"', 'lobster', 'red', '\"', 'on', 'cooking', '.', 'mating', 'occurs', 'in', 'the', 'summer', ',', 'producing', 'eggs', 'which', 'are', 'carried', 'by', 'the', 'females', 'for', 'up', 'to', 'a', 'year', 'before', 'hatching', 'into', '<unk>', 'larvae', '.', 'homarus', 'gammarus', 'is', 'a', 'highly', 'esteemed', 'food', ',', 'and', 'is', 'widely', 'caught', 'using', 'lobster', 'pots', ',', 'mostly', 'around', 'the', 'british', 'isles', '.']\n",
      "['<START>', 'homarus', 'gammarus', ',', 'known', 'as', 'the', 'european', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'atlantic', 'ocean', ',', 'mediterranean', 'sea', 'and', 'parts', 'of', 'the', 'black', 'sea', '.', 'it', 'is', 'closely', 'related', 'to', 'the', 'american', 'lobster', ',', 'h.', 'americanus', '.', 'it', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'in', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', '\"', 'lobster', 'red', '\"', 'on', 'cooking', '.', 'mating', 'occurs', 'in', 'the', 'summer', ',', 'producing', 'eggs', 'which', 'are', 'carried', 'by', 'the', 'females', 'for', 'up', 'to', 'a', 'year', 'before', 'hatching', 'into', '<unk>', 'larvae', '.', 'homarus', 'gammarus', 'is', 'a', 'highly', 'esteemed', 'food', ',', 'and', 'is', 'widely', 'caught', 'using', 'lobster', 'pots', ',', 'mostly', 'around', 'the', 'british', 'isles', '.']\n",
      "['<START>', '<START>', 'homarus', 'gammarus', ',', 'known', 'as', 'the', 'european', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'atlantic', 'ocean', ',', 'mediterranean', 'sea', 'and', 'parts', 'of', 'the', 'black', 'sea', '.', 'it', 'is', 'closely', 'related', 'to', 'the', 'american', 'lobster', ',', 'h.', 'americanus', '.', 'it', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'in', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', '\"', 'lobster', 'red', '\"', 'on', 'cooking', '.', 'mating', 'occurs', 'in', 'the', 'summer', ',', 'producing', 'eggs', 'which', 'are', 'carried', 'by', 'the', 'females', 'for', 'up', 'to', 'a', 'year', 'before', 'hatching', 'into', '<unk>', 'larvae', '.', 'homarus', 'gammarus', 'is', 'a', 'highly', 'esteemed', 'food', ',', 'and', 'is', 'widely', 'caught', 'using', 'lobster', 'pots', ',', 'mostly', 'around', 'the', 'british', 'isles', '.']\n",
      "['<START>', 'homarus', 'gammarus', 'is', 'a', 'large', '<unk>', ',', 'with', 'a', 'body', 'length', 'up', 'to', '60', 'centimetres', '(', '24', 'in', ')', 'and', 'weighing', 'up', 'to', '5', '–', '6', 'kilograms', '(', '11', '–', '13', 'lb', ')', ',', 'although', 'the', 'lobsters', 'caught', 'in', 'lobster', 'pots', 'are', 'usually', '23', '–', '38', 'cm', '(', '9', '–', '15', 'in', ')', 'long', 'and', 'weigh', '0', '@.@', '7', '–', '2', '@.@', '2', 'kg', '(', '1', '@.@', '5', '–', '4', '@.@', '9', 'lb', ')', '.', 'like', 'other', 'crustaceans', ',', 'lobsters', 'have', 'a', 'hard', '<unk>', 'which', 'they', 'must', 'shed', 'in', 'order', 'to', 'grow', ',', 'in', 'a', 'process', 'called', '<unk>', '(', '<unk>', ')', '.', 'this', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('data/wikitext-2/wiki.valid.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    valid_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    valid_sents = [s for s in valid_sents if len(s)>0 and s[0] != '=']\n",
    "\n",
    "uni_valid_sents = pad_sents(valid_sents, 1)\n",
    "bi_valid_sents = pad_sents(valid_sents, 2)\n",
    "tri_valid_sents = pad_sents(valid_sents, 3)\n",
    "\n",
    "print(bi_valid_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tlFXWbkjC-a"
   },
   "outputs": [],
   "source": [
    "def smooth_ngram_prob(ngram, k, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        k: float \n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dicV = len(unigram_dic) + 1 : dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        s_prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    s_prob = 0\n",
    "    V = len(unigram_dic) + 1 \n",
    "    ### YOUR CODE HERE\\、\n",
    "    \n",
    "    ngram_tuple = tuple(ngram)\n",
    "\n",
    "    if len(ngram) == 1:\n",
    "        #prob = count(u) / c\n",
    "        # to find number of occurrence the unigram list appears from the unigram_dict\n",
    "        s_prob = (unigram_dic.get(ngram_tuple, 0) + k) / (num_words + k*V)\n",
    "        \n",
    "    elif len(ngram) == 2:\n",
    "        # count(v,u)/ count(v)\n",
    "        s_prob = (bigram_dic.get(ngram_tuple, 0) + k) / (unigram_dic.get(tuple([ngram_tuple[0]]), 0) + k*V)\n",
    "    \n",
    "    elif len(ngram) ==3:\n",
    "        #count(w,v,u)/count(w,v)\n",
    "        s_prob = (trigram_dic.get(ngram_tuple, 0)+k) / (bigram_dic.get(tuple(ngram_tuple[0:2]), 0) + k*V)\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return s_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dMIkPRhLjC-f",
    "outputId": "e02bfcfc-3da8-4daf-d0cd-7ca16908e18e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.311982452086402e-05"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ~ 9.31e-05\n",
    "smooth_ngram_prob(ngrams[0], 0.5, num_words, unigram_dict, bigram_dict, trigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSxCxqZrjC-l"
   },
   "outputs": [],
   "source": [
    "def perplexity(n, k, num_words, valid_sents, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    compute the perplexity of valid_sents\n",
    "    params:\n",
    "        n: int --- n-gram model you choose. \n",
    "        k: float --- smoothing parameter.\n",
    "        num_words: int --- total number of words in the traning set.\n",
    "        valid_sents: list[list[str]] --- list of sentences.\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        ppl: float --- perplexity of valid_sents\n",
    "    '''\n",
    "    ppl = None\n",
    "    ### YOUR CODE HERE\n",
    "    prob_sentence = 0;\n",
    "    \n",
    "    # get sentence\n",
    "    for sents in valid_sents:\n",
    "        sum_prob = 0\n",
    "        for i in range(len(sents)-n+1):\n",
    "                \n",
    "            # get ngrams\n",
    "            ngram = sents[i:i+n]    \n",
    "            # get prob of ngram from list of words\n",
    "            prob = smooth_ngram_prob(ngram, k, num_words, unigram_dic, bigram_dic, trigram_dic)\n",
    "            log_prob = math.log(prob,2)\n",
    "            sum_prob += log_prob  \n",
    "                \n",
    "        # summation of log2 p(x^(j) over test set D'\n",
    "        prob_sentence += sum_prob \n",
    "    \n",
    "    # get len of each sent\n",
    "    ea_sent = [len(l) for l in valid_sents]\n",
    "\n",
    "    # sum the len of each sent \n",
    "    len_ts = sum(ea_sent) \n",
    "    l = (1/len_ts)*prob_sentence\n",
    "    ppl = 2**(-l)                            \n",
    "             \n",
    "    ### END OF YOUR CODE\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tRcux0YwjC-q",
    "outputId": "43ee0a67-b4d3-47a3-ccb7-6ef19bc03e33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840.7347306258201"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ~ 840\n",
    "perplexity(1, 0.1, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ON9aSS1IjC-y",
    "outputId": "a0259ee8-ef7e-4158-ea1c-f57e34694425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n = 1 the best k that gives the smallest perp:  (0.1, 840.7347306258201)\n",
      "for n = 2 the best k that gives the smallest perp:  (0.1, 788.5223577024464)\n",
      "for n = 3 the best k that gives the smallest perp:  (0.1, 5079.078804814675)\n"
     ]
    }
   ],
   "source": [
    "n = [1,2,3]\n",
    "k = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# for each n, find the best k (smallest perp)\n",
    "### YOUR CODE HERE\n",
    "d = {}\n",
    "for ngram in n:\n",
    "    # iterate over the k values \n",
    "    for kth in k:\n",
    "        ppl = perplexity(ngram, kth, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict)\n",
    "        d[kth] = ppl\n",
    "#         print('dis', ngram, kth, ppl)\n",
    "    min_val = min(d.items(), key=lambda x: x[1])\n",
    "    print('for n =',ngram,'the best k that gives the smallest perp: ', min_val)\n",
    "        \n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3o_yjh9lCdXg"
   },
   "source": [
    "### Question 4 [code]\n",
    "\n",
    "Evaluate the perplexity of the test data **test_sents** based on the best n-gram model and $k$ you have found on the validation data (Q 3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "i7hGcXgCCdXg",
    "outputId": "ca0f1599-59f9-437f-d0d1-d287fa081920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robert', '<unk>', 'is', 'an', 'english', 'film', ',', 'television', 'and', 'theatre', 'actor', '.', 'he', 'had', 'a', 'guest', '@-@', 'starring', 'role', 'on', 'the', 'television', 'series', 'the', 'bill', 'in', '2000', '.', 'this', 'was', 'followed', 'by', 'a', 'starring', 'role', 'in', 'the', 'play', 'herons', 'written', 'by', 'simon', 'stephens', ',', 'which', 'was', 'performed', 'in', '2001', 'at', 'the', 'royal', 'court', 'theatre', '.', 'he', 'had', 'a', 'guest', 'role', 'in', 'the', 'television', 'series', 'judge', 'john', '<unk>', 'in', '2002', '.', 'in', '2004', '<unk>', 'landed', 'a', 'role', 'as', '\"', 'craig', '\"', 'in', 'the', 'episode', '\"', 'teddy', \"'s\", 'story', '\"', 'of', 'the', 'television', 'series', 'the', 'long', 'firm', ';', 'he', 'starred', 'alongside', 'actors', 'mark', 'strong', 'and', 'derek', 'jacobi', '.', 'he', 'was', 'cast', 'in', 'the', '2005', 'theatre', 'productions', 'of', 'the', 'philip', 'ridley', 'play', 'mercury', 'fur', ',', 'which', 'was', 'performed', 'at', 'the', 'drum', 'theatre', 'in', 'plymouth', 'and', 'the', '<unk>', '<unk>', 'factory', 'in', 'london', '.', 'he', 'was', 'directed', 'by', 'john', '<unk>', 'and', 'starred', 'alongside', 'ben', '<unk>', ',', 'shane', '<unk>', ',', 'harry', 'kent', ',', 'fraser', '<unk>', ',', 'sophie', 'stanton', 'and', 'dominic', 'hall', '.']\n",
      "['<START>', 'robert', '<unk>', 'is', 'an', 'english', 'film', ',', 'television', 'and', 'theatre', 'actor', '.', 'he', 'had', 'a', 'guest', '@-@', 'starring', 'role', 'on', 'the', 'television', 'series', 'the', 'bill', 'in', '2000', '.', 'this', 'was', 'followed', 'by', 'a', 'starring', 'role', 'in', 'the', 'play', 'herons', 'written', 'by', 'simon', 'stephens', ',', 'which', 'was', 'performed', 'in', '2001', 'at', 'the', 'royal', 'court', 'theatre', '.', 'he', 'had', 'a', 'guest', 'role', 'in', 'the', 'television', 'series', 'judge', 'john', '<unk>', 'in', '2002', '.', 'in', '2004', '<unk>', 'landed', 'a', 'role', 'as', '\"', 'craig', '\"', 'in', 'the', 'episode', '\"', 'teddy', \"'s\", 'story', '\"', 'of', 'the', 'television', 'series', 'the', 'long', 'firm', ';', 'he', 'starred', 'alongside', 'actors', 'mark', 'strong', 'and', 'derek', 'jacobi', '.', 'he', 'was', 'cast', 'in', 'the', '2005', 'theatre', 'productions', 'of', 'the', 'philip', 'ridley', 'play', 'mercury', 'fur', ',', 'which', 'was', 'performed', 'at', 'the', 'drum', 'theatre', 'in', 'plymouth', 'and', 'the', '<unk>', '<unk>', 'factory', 'in', 'london', '.', 'he', 'was', 'directed', 'by', 'john', '<unk>', 'and', 'starred', 'alongside', 'ben', '<unk>', ',', 'shane', '<unk>', ',', 'harry', 'kent', ',', 'fraser', '<unk>', ',', 'sophie', 'stanton', 'and', 'dominic', 'hall', '.']\n",
      "['<START>', '<START>', 'robert', '<unk>', 'is', 'an', 'english', 'film', ',', 'television', 'and', 'theatre', 'actor', '.', 'he', 'had', 'a', 'guest', '@-@', 'starring', 'role', 'on', 'the', 'television', 'series', 'the', 'bill', 'in', '2000', '.', 'this', 'was', 'followed', 'by', 'a', 'starring', 'role', 'in', 'the', 'play', 'herons', 'written', 'by', 'simon', 'stephens', ',', 'which', 'was', 'performed', 'in', '2001', 'at', 'the', 'royal', 'court', 'theatre', '.', 'he', 'had', 'a', 'guest', 'role', 'in', 'the', 'television', 'series', 'judge', 'john', '<unk>', 'in', '2002', '.', 'in', '2004', '<unk>', 'landed', 'a', 'role', 'as', '\"', 'craig', '\"', 'in', 'the', 'episode', '\"', 'teddy', \"'s\", 'story', '\"', 'of', 'the', 'television', 'series', 'the', 'long', 'firm', ';', 'he', 'starred', 'alongside', 'actors', 'mark', 'strong', 'and', 'derek', 'jacobi', '.', 'he', 'was', 'cast', 'in', 'the', '2005', 'theatre', 'productions', 'of', 'the', 'philip', 'ridley', 'play', 'mercury', 'fur', ',', 'which', 'was', 'performed', 'at', 'the', 'drum', 'theatre', 'in', 'plymouth', 'and', 'the', '<unk>', '<unk>', 'factory', 'in', 'london', '.', 'he', 'was', 'directed', 'by', 'john', '<unk>', 'and', 'starred', 'alongside', 'ben', '<unk>', ',', 'shane', '<unk>', ',', 'harry', 'kent', ',', 'fraser', '<unk>', ',', 'sophie', 'stanton', 'and', 'dominic', 'hall', '.']\n"
     ]
    }
   ],
   "source": [
    "with open('data/wikitext-2/wiki.test.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    test_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    test_sents = [s for s in test_sents if len(s)>0 and s[0] != '=']\n",
    "\n",
    "uni_test_sents = pad_sents(test_sents, 1)\n",
    "bi_test_sents = pad_sents(test_sents, 2)\n",
    "tri_test_sents = pad_sents(test_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5J1ILpGMjC_E",
    "outputId": "b9376c3a-afd0-457f-e65d-0541f785c399"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715.5058643689783"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "perplexity(2, 0.1, num_words,bi_test_sents, unigram_dict, bigram_dict, trigram_dict)\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePSI8RDWCdXj"
   },
   "source": [
    "## Neural Language Model (RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkoTco_jCdXj"
   },
   "source": [
    "<img src=\"LM.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "We will create a LSTM language model as shown in figure and train it on the Wikitext-2 dataset. \n",
    "The data generators (train\\_iter, valid\\_iter, test\\_iter) have been provided. \n",
    "The word embeddings together with the parameters in the LSTM model will be learned from scratch.\n",
    "\n",
    "[Pytorch](https://pytorch.org/tutorials/) and [torchtext](https://torchtext.readthedocs.io/en/latest/index.html#) are required in this part. Do not make any changes to the provided code unless you are requested to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHJ8kBBAjC_N"
   },
   "source": [
    "### Question 5 [code]\n",
    "- Implement the ``__init__`` function in ``LangModel`` class.\n",
    "- Implement the ``forward`` function in ``LangModel`` class.\n",
    "- Complete the training code in ``train`` function.\n",
    "    Then complete the testing code in  ``test`` function and \n",
    "    compute the perplexity of the test data ``test_iter``. The test perplexity should be below 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "tF2GCRdFCdXk",
    "outputId": "74e891cd-2caa-4122-bb71-50f35191cd14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fa470245f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import WikiText2\n",
    "from torch import nn, optim\n",
    "from torchtext import data\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "torch.manual_seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lt6JBdSnCdXn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    '''Tokenize a string to words'''\n",
    "    return word_tokenize(text)\n",
    "\n",
    "START = '<START>'\n",
    "STOP = '<STOP>'\n",
    "#Load and split data into three parts\n",
    "TEXT = data.Field(lower=True, tokenize=tokenizer, init_token=START, eos_token=STOP)\n",
    "train, valid, test = WikiText2.splits(TEXT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KHwu4VMVCdXq",
    "outputId": "dc865b32-f900-4fb3-a214-7c56eb615bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28905\n"
     ]
    }
   ],
   "source": [
    "#Build a vocabulary from the train dataset\n",
    "TEXT.build_vocab(train)\n",
    "print('Vocabulary size:', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ua9e-OBMCdXs"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# the length of a piece of text feeding to the RNN layer\n",
    "BPTT_LEN = 32           \n",
    "# train, validation, test data\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                bptt_len=BPTT_LEN,\n",
    "                                                                repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6_Cd8shXCdXy",
    "outputId": "7832941f-7da3-451d-9941-a42882f9b940",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text tensor torch.Size([32, 64])\n",
      "Size of target tensor torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "#Generate a batch of train data\n",
    "batch = next(iter(train_iter))\n",
    "text, target = batch.text, batch.target\n",
    "# print(batch.dataset[0].text[:32])\n",
    "# print(text[0:3],target[:3])\n",
    "print('Size of text tensor',text.size())\n",
    "print('Size of target tensor',target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETYx5ovYjC_t"
   },
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, lang_config):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.vocab_size = lang_config['vocab_size']\n",
    "        self.emb_size = lang_config['emb_size']\n",
    "        self.hidden_size = lang_config['hidden_size']\n",
    "        self.num_layer = lang_config['num_layer']\n",
    "        \n",
    "        self.embedding = None\n",
    "        self.rnn = None\n",
    "        self.linear = None\n",
    "        \n",
    "        ### TODO: \n",
    "        ###    1. Initialize 'self.embedding' with nn.Embedding function and 2 variables we have initialized for you\n",
    "        ###    2. Initialize 'self.rnn' with nn.LSTM function and 3 variables we have initialized for you\n",
    "        ###    3. Initialize 'self.linear' with nn.Linear function and 2 variables we have initialized for you\n",
    "        ### Reference:\n",
    "        ###        https://pytorch.org/docs/stable/nn.html\n",
    "        \n",
    "        ### YOUR CODE HERE (3 lines)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.rnn = nn.LSTM(self.emb_size, self.hidden_size,self.num_layer)\n",
    "        self.linear= nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "        \n",
    "    def forward(self, batch_sents, hidden=None):\n",
    "        '''\n",
    "        params:\n",
    "            batch_sents: torch.LongTensor of shape (sequence_len, batch_size)\n",
    "        return:\n",
    "            normalized_score: torch.FloatTensor of shape (sequence_len, batch_size, vocab_size)\n",
    "        '''\n",
    "        normalized_score = None\n",
    "        hidden = hidden\n",
    "        ### TODO:\n",
    "        ###      1. Feed the batch_sents to self.embedding  \n",
    "        ###      2. Feed the embeddings to self.rnn. Remember to pass \"hidden\" into self.rnn, even if it is None. But we will \n",
    "        ###         use \"hidden\" when implementing greedy search.\n",
    "        ###      3. Apply linear transformation to the output of self.rnn\n",
    "        ###      4. Apply 'F.log_softmax' to the output of linear transformation\n",
    "        ###\n",
    "        ### YOUR CODE HERE \n",
    "        \n",
    "        embeddings = self.embedding(batch_sents)\n",
    "        out, hidden = self.rnn(embeddings, hidden)\n",
    "        output = self.linear(out)\n",
    "        normalized_score = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "        return normalized_score, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7JAW3FAjC_z"
   },
   "outputs": [],
   "source": [
    "def train(model, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs):\n",
    "    # num_epoch: entire data set is passed forward and backward thru RNN once\n",
    "    for n in range(num_epochs):\n",
    "        print(n)\n",
    "        train_loss = 0\n",
    "        target_num = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_iter:\n",
    "            \n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            loss = None\n",
    "            \n",
    "            ### we don't consider \"hidden\" here. So according to the default setting, \"hidden\" will be None\n",
    "            ### YOU CODE HERE (~5 lines)\n",
    "\n",
    "            #clear gradients before each instance \n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(text)\n",
    "\n",
    "            # compute loss, grad and update params using optimizer.step()\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()     \n",
    "            \n",
    "            ### END OF YOUR CODE\n",
    "            ##########################################\n",
    "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        train_loss /= target_num\n",
    "\n",
    "        # monitor the loss of all the predictions\n",
    "        val_loss = 0\n",
    "        target_num = 0\n",
    "        model.eval()\n",
    "        for batch in valid_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            \n",
    "            prediction,_ = model(text)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "            \n",
    "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "        val_loss /= target_num\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6qOpFZjjC_5"
   },
   "outputs": [],
   "source": [
    "def test(model, vocab_size, criterion, test_iter):\n",
    "    '''\n",
    "    params: \n",
    "        model: LSTM model\n",
    "        test_iter: test data\n",
    "    return:\n",
    "        ppl: perplexity \n",
    "    '''\n",
    "    ppl = None\n",
    "    test_loss = 0\n",
    "    target_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "\n",
    "            prediction,_ = model(text, hidden=None)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        test_loss /= target_num\n",
    "        \n",
    "        ### Compute perplexity according to \"test_loss\"\n",
    "        ### Hint: Consider how the loss is computed.\n",
    "        ### YOUR CODE HERE(1 line)\n",
    "        ppl = math.exp(test_loss)\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PN6LdYhAjDAA"
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "config = {'vocab_size':vocab_size,\n",
    "         'emb_size':128,\n",
    "         'hidden_size':128,\n",
    "         'num_layer':1}\n",
    "\n",
    "LM = LangModel(config)\n",
    "LM = LM.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean')\n",
    "optimizer = optim.Adam(LM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T4Kq12w8jDAE",
    "outputId": "2b9b922d-44ee-4a09-ee28-2a0f55aa8fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 1, Training Loss: 6.0688, Validation Loss: 5.1867\n",
      "1\n",
      "Epoch: 2, Training Loss: 5.4029, Validation Loss: 4.9612\n",
      "2\n",
      "Epoch: 3, Training Loss: 5.1289, Validation Loss: 4.8638\n",
      "3\n",
      "Epoch: 4, Training Loss: 4.9559, Validation Loss: 4.8165\n",
      "4\n",
      "Epoch: 5, Training Loss: 4.8306, Validation Loss: 4.7866\n",
      "5\n",
      "Epoch: 6, Training Loss: 4.7318, Validation Loss: 4.7675\n",
      "6\n",
      "Epoch: 7, Training Loss: 4.6504, Validation Loss: 4.7564\n",
      "7\n",
      "Epoch: 8, Training Loss: 4.5805, Validation Loss: 4.7505\n",
      "8\n",
      "Epoch: 9, Training Loss: 4.5198, Validation Loss: 4.7475\n",
      "9\n",
      "Epoch: 10, Training Loss: 4.4664, Validation Loss: 4.7483\n"
     ]
    }
   ],
   "source": [
    "train(LM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACvUYrE7jDAI"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0b2aba212988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# < 150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-5b6968ab780c>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, vocab_size, criterion, test_iter)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-8ffa48648e85>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, batch_sents, hidden)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mnormalized_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m### END OF YOUR CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1536\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# < 150\n",
    "test(LM, vocab_size, criterion, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noU6xo_jjDAM"
   },
   "source": [
    "### Question 6 [code]\n",
    "When we use trained language model to generate a sentence given a start token, we can choose either ``greedy search`` or ``beam search``. \n",
    "<img src=\"greedy.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "As shown above, ``greedy search`` algorithm will pick the token which has the highest probability and feed it to the language model as input in the next time step. The model will generate ``max_len`` number of tokens at most.\n",
    "\n",
    "- Implement ``word_greedy_search``\n",
    "- **[optional]** Implement ``word_beam_search`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AkT-5OCsjDAN",
    "outputId": "bd103241-4b2a-40ea-f655-007a4cbd80a2"
   },
   "outputs": [],
   "source": [
    "def word_greedy_search(model, start_token, max_len):\n",
    "    '''\n",
    "    param:\n",
    "        model: nn.Module --- language model\n",
    "        start_token: str --- e.g. 'he'\n",
    "        max_len: int --- max number of tokens generated\n",
    "    return:\n",
    "        strings: list[str] --- list of tokens, e.g., ['he', 'was', 'a', 'member', 'of',...]\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Defines a vocabulary object that will be used to numericalize a field.\n",
    "    ID = TEXT.vocab.stoi[start_token]\n",
    "    strings = [start_token]\n",
    "    hidden = None\n",
    "    \n",
    "    ### You may find TEXT.vocab.itos useful.\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size, ID.\n",
    "    leading = torch.ones(1,1)\n",
    "    leading = leading.long().to(device) * ID\n",
    "\n",
    "    # iterate through input\n",
    "    for i in range(max_len):\n",
    "        outputs, hidden = model(leading, hidden)\n",
    "        leading = torch.argmax(outputs[-1,:,:], dim=-1)\n",
    "        inp = leading.cpu().detach().numpy()[0]\n",
    "        print(inp)\n",
    "        output = TEXT.vocab.itos[inp]\n",
    "        print(output)\n",
    "#         print(output)\n",
    "        if strings[-1] == '<eos>': \n",
    "            break\n",
    "        else:\n",
    "            strings.append(output)\n",
    "            \n",
    "        leading.unsqueeze_(0)\n",
    "#     strings.pop()\n",
    "\n",
    "    ### END OF YOUR CODE \n",
    "    return strings\n",
    "\n",
    "# word_greedy_search(LM, 'he', 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocpx3-HZjDAR"
   },
   "outputs": [],
   "source": [
    "# BeamNode = namedtuple('BeamNode', ['prev_node', 'prev_hidden', 'wordID', 'score', 'length'])\n",
    "# LMNode = namedtuple('LMNode', ['sent', 'score'])\n",
    "\n",
    "def word_beam_search(model, start_token, max_len, beam_size):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTNwk0rTjDAX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18816\n",
      "discreet\n",
      "14429\n",
      "fuller\n",
      "28685\n",
      "vow\n",
      "19490\n",
      "lombardi\n",
      "14420\n",
      "française\n",
      "815\n",
      "mexico\n",
      "22257\n",
      "magnussen\n",
      "14273\n",
      "deccan\n",
      "26346\n",
      "khitans\n",
      "13730\n",
      "refinery\n",
      "12830\n",
      "scientologists\n",
      "22100\n",
      "kinnear\n",
      "23227\n",
      "spectroscopic\n",
      "13270\n",
      "despatched\n",
      "1090\n",
      "california\n",
      "17394\n",
      "labels\n",
      "10347\n",
      "270\n",
      "912\n",
      "lake\n",
      "22378\n",
      "mips\n",
      "24322\n",
      "bacolas\n",
      "9579\n",
      "deposition\n",
      "10577\n",
      "hum\n",
      "6034\n",
      "exeter\n",
      "14862\n",
      "sandra\n",
      "24341\n",
      "balkans\n",
      "2970\n",
      "initiative\n",
      "16898\n",
      "cremation\n",
      "20242\n",
      "taiwanese\n",
      "4688\n",
      "fourteen\n",
      "9608\n",
      "exploits\n",
      "25824\n",
      "grenville\n",
      "6234\n",
      "poetic\n",
      "16804\n",
      "chronology\n",
      "21082\n",
      "carnation\n",
      "19120\n",
      "gould\n",
      "26342\n",
      "kgul\n",
      "12021\n",
      "residing\n",
      "19855\n",
      "prehistory\n",
      "16804\n",
      "chronology\n",
      "3858\n",
      "machines\n",
      "22650\n",
      "pathologist\n",
      "21109\n",
      "ceratopsidae\n",
      "6136\n",
      "abundant\n",
      "9983\n",
      "conspirators\n",
      "20308\n",
      "transaction\n",
      "18570\n",
      "campers\n",
      "19018\n",
      "figured\n",
      "9672\n",
      "judgment\n",
      "23835\n",
      "1610\n",
      "8141\n",
      "ticket\n",
      "5881\n",
      "fastra\n",
      "16165\n",
      "rites\n",
      "16495\n",
      "1603\n",
      "1425\n",
      "stating\n",
      "3873\n",
      "roots\n",
      "16360\n",
      "tuning\n",
      "27941\n",
      "showered\n",
      "9981\n",
      "congressman\n",
      "6216\n",
      "lions\n",
      "23957\n",
      "639\n",
      "2253\n",
      "eyes\n",
      "1842\n",
      "assault\n",
      "27484\n",
      "raphinae\n",
      "22872\n",
      "rancid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'discreet',\n",
       " 'fuller',\n",
       " 'vow',\n",
       " 'lombardi',\n",
       " 'française',\n",
       " 'mexico',\n",
       " 'magnussen',\n",
       " 'deccan',\n",
       " 'khitans',\n",
       " 'refinery',\n",
       " 'scientologists',\n",
       " 'kinnear',\n",
       " 'spectroscopic',\n",
       " 'despatched',\n",
       " 'california',\n",
       " 'labels',\n",
       " '270',\n",
       " 'lake',\n",
       " 'mips',\n",
       " 'bacolas',\n",
       " 'deposition',\n",
       " 'hum',\n",
       " 'exeter',\n",
       " 'sandra',\n",
       " 'balkans',\n",
       " 'initiative',\n",
       " 'cremation',\n",
       " 'taiwanese',\n",
       " 'fourteen',\n",
       " 'exploits',\n",
       " 'grenville',\n",
       " 'poetic',\n",
       " 'chronology',\n",
       " 'carnation',\n",
       " 'gould',\n",
       " 'kgul',\n",
       " 'residing',\n",
       " 'prehistory',\n",
       " 'chronology',\n",
       " 'machines',\n",
       " 'pathologist',\n",
       " 'ceratopsidae',\n",
       " 'abundant',\n",
       " 'conspirators',\n",
       " 'transaction',\n",
       " 'campers',\n",
       " 'figured',\n",
       " 'judgment',\n",
       " '1610',\n",
       " 'ticket',\n",
       " 'fastra',\n",
       " 'rites',\n",
       " '1603',\n",
       " 'stating',\n",
       " 'roots',\n",
       " 'tuning',\n",
       " 'showered',\n",
       " 'congressman',\n",
       " 'lions',\n",
       " '639',\n",
       " 'eyes',\n",
       " 'assault',\n",
       " 'raphinae',\n",
       " 'rancid']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_greedy_search(LM, 'he', 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfT4rYSZjDAf"
   },
   "outputs": [],
   "source": [
    "word_beam_search(LM, 'he', 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2c7qJZycjDAi"
   },
   "source": [
    "# char-level LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7eoSEgzjDAk"
   },
   "source": [
    "### Question 7 [code]\n",
    "- Implement ``char_tokenizer``\n",
    "- Implement ``CharLangModel``, ``char_train``, ``char_test``\n",
    "- Implement ``char_greedy_search``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9Elh6mmjDAl"
   },
   "outputs": [],
   "source": [
    "def char_tokenizer(string):\n",
    "    '''\n",
    "    param:\n",
    "        string: str --- e.g. \"I love this assignment\"\n",
    "    return:\n",
    "        char_list: list[str] --- e.g. ['I', 'l', 'o', 'v', 'e', ' ', 't', 'h', 'i', 's', ...]\n",
    "    '''\n",
    "    char_list = [START]\n",
    "    char_list.extend(string)\n",
    "    ### END OF YOUR CODE\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIBgsUh9jDAp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = 'test test test'\n",
    "char_tokenizer(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EG9y3NMtjDAu"
   },
   "outputs": [],
   "source": [
    "CHAR_TEXT = data.Field(lower=True, tokenize=char_tokenizer ,init_token='<START>', eos_token='<STOP>')\n",
    "ctrain, cvalid, ctest = WikiText2.splits(CHAR_TEXT)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogynm0sJjDAz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 248\n"
     ]
    }
   ],
   "source": [
    "CHAR_TEXT.build_vocab(ctrain)\n",
    "print('Vocabulary size:', len(CHAR_TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Qi0lCeWjDA4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# the length of a piece of text feeding to the RNN layer\n",
    "BPTT_LEN = 128        \n",
    "# train, validation, test data\n",
    "ctrain_iter, cvalid_iter, ctest_iter = data.BPTTIterator.splits((ctrain, cvalid, ctest),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                bptt_len=BPTT_LEN,\n",
    "                                                                repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAnJn_EujDBA"
   },
   "outputs": [],
   "source": [
    "class CharLangModel(nn.Module):\n",
    "    def __init__(self, lang_config):\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        super(CharLangModel, self).__init__()\n",
    "        self.vocab_size = lang_config['vocab_size']\n",
    "        self.emb_size = lang_config['emb_size']\n",
    "        self.hidden_size = lang_config['hidden_size']\n",
    "        self.num_layer = lang_config['num_layer']\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.rnn = nn.LSTM(self.emb_size, self.hidden_size,self.num_layer)\n",
    "        self.linear= nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, batch_sents, hidden=None):\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        normalized_score = None\n",
    "        hidden = hidden\n",
    "        \n",
    "        embeddings = self.embedding(batch_sents)\n",
    "        out, hidden = self.rnn(embeddings, hidden)\n",
    "        output = self.linear(out)\n",
    "        normalized_score = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "        return normalized_score, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BzTo8SyxjDBE"
   },
   "outputs": [],
   "source": [
    "def char_train(model, train_iter, valid_iter, criterion, optimizer, vocab_size, num_epochs):\n",
    "    # num_epoch: entire data set is passed forward and backward thru RNN once\n",
    "    for n in range(num_epochs):\n",
    "        print(n)\n",
    "        train_loss = 0\n",
    "        target_num = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_iter:\n",
    "            \n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            loss = None\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(text, hidden=None)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()     \n",
    "            \n",
    "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        train_loss /= target_num\n",
    "\n",
    "        # monitor the loss of all the predictions\n",
    "        val_loss = 0\n",
    "        target_num = 0\n",
    "        model.eval()\n",
    "        for batch in valid_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            \n",
    "            prediction,_ = model(text, hidden=None)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "            \n",
    "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "        val_loss /= target_num\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzw79JTAjDBR"
   },
   "outputs": [],
   "source": [
    "def char_test(model, vocab_size, test_iter, criterion):\n",
    "    ppl = None\n",
    "    test_loss = 0\n",
    "    target_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "\n",
    "            prediction, hidden = model(text, hidden=None)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        test_loss /= target_num\n",
    "\n",
    "        ppl = math.exp(test_loss)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8WDHOgdjDBV"
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "char_vocab_size = len(CHAR_TEXT.vocab)\n",
    "\n",
    "config = {'vocab_size':char_vocab_size,\n",
    "         'emb_size':128,\n",
    "         'hidden_size':128,\n",
    "         'num_layer':1}\n",
    "\n",
    "CLM = CharLangModel(config)\n",
    "CLM = CLM.to(device)\n",
    "\n",
    "char_criterion = nn.NLLLoss(reduction='mean')\n",
    "char_optimizer = optim.Adam(CLM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldSikvjBjDBp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 1, Training Loss: 1.8334, Validation Loss: 1.5410\n",
      "1\n",
      "Epoch: 2, Training Loss: 1.5417, Validation Loss: 1.4394\n",
      "2\n",
      "Epoch: 3, Training Loss: 1.4703, Validation Loss: 1.3956\n",
      "3\n",
      "Epoch: 4, Training Loss: 1.4337, Validation Loss: 1.3699\n",
      "4\n",
      "Epoch: 5, Training Loss: 1.4101, Validation Loss: 1.3531\n",
      "5\n",
      "Epoch: 6, Training Loss: 1.3936, Validation Loss: 1.3410\n",
      "6\n",
      "Epoch: 7, Training Loss: 1.3812, Validation Loss: 1.3319\n",
      "7\n",
      "Epoch: 8, Training Loss: 1.3714, Validation Loss: 1.3245\n",
      "8\n",
      "Epoch: 9, Training Loss: 1.3634, Validation Loss: 1.3183\n",
      "9\n",
      "Epoch: 10, Training Loss: 1.3569, Validation Loss: 1.3132\n"
     ]
    }
   ],
   "source": [
    "char_train(CLM, ctrain_iter, cvalid_iter, char_criterion, char_optimizer, char_vocab_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6AKNfKHjDBu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.683521135213296"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <10\n",
    "char_test(CLM, char_vocab_size, ctest_iter, char_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nw-VfmL7jDB8"
   },
   "outputs": [],
   "source": [
    "def char_greedy_search(model, start_token, max_len):\n",
    "    '''\n",
    "    param:\n",
    "        model: nn.Module --- language model\n",
    "        start_token: str --- e.g. 'h'\n",
    "        max_len: int --- max number of tokens generated\n",
    "    return:\n",
    "        strings: list[str] --- list of tokens, e.g., ['h', 'e', ' ', 'i', 's',...]\n",
    "    '''   \n",
    "    model.eval()\n",
    "    ID = CHAR_TEXT.vocab.stoi[start_token]\n",
    "    strings = [start_token]\n",
    "    hidden = None\n",
    "    \n",
    "    ### You may find CHAR_TEXT.vocab.itos useful.\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    last_one_hot = torch.ones(1,1)\n",
    "    last_one_hot = last_one_hot.long().to(device) * ID\n",
    "\n",
    "    # iterate through input\n",
    "    for i in range(max_len):\n",
    "        outputs, hidden = model(last_one_hot, hidden)\n",
    "        last_one_hot = torch.argmax(outputs[-1,:,:], dim=-1)\n",
    "        output = TEXT.vocab.itos[last_one_hot.cpu().detach().numpy()[0]]\n",
    "#         print(output)\n",
    "        strings.append(output)\n",
    "\n",
    "        if strings[-1] == '<eos>': \n",
    "            break\n",
    "        last_one_hot.unsqueeze_(0)\n",
    "    strings.pop()\n",
    "\n",
    "    ### END OF YOUR CODE \n",
    "    return strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cernoBq6CdX9"
   },
   "source": [
    "### Requirements:\n",
    "- This is an individual report.\n",
    "- Complete the code using Python.\n",
    "- List students with whom you have discussed if there are any.\n",
    "- Follow the honor code strictly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDOUuC7-CdX-"
   },
   "source": [
    "### Free GPU Resources\n",
    "We suggest that you run neural language models on machines with GPU(s). Google provides the free online platform [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use as common packages have been  pre-installed. Google users can have access to a Tesla T4 GPU (approximately 15G memory). Note that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM.\n",
    "\n",
    "It is convenient to upload local Jupyter Notebook files and data to Colab, please refer to the [tutorial](https://colab.research.google.com/notebooks/io.ipynb). \n",
    "\n",
    "In addition, Microsoft also provides the online platform [Azure Notebooks](https://notebooks.azure.com/help/introduction) for research of data science and machine learning, there are free trials for new users with credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFP5U22kjDCE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "mini_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
